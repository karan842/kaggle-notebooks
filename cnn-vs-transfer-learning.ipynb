{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/karan842/cnn-vs-transfer-learning?scriptVersionId=90704112\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Is it CATðŸˆ or DOGðŸ• ðŸ¤”ðŸ’­","metadata":{"execution":{"iopub.status.busy":"2022-02-01T14:23:26.741871Z","iopub.execute_input":"2022-02-01T14:23:26.742428Z","iopub.status.idle":"2022-02-01T14:23:26.780003Z","shell.execute_reply.started":"2022-02-01T14:23:26.742311Z","shell.execute_reply":"2022-02-01T14:23:26.779022Z"}}},{"cell_type":"markdown","source":"## Image detection with the help of Neural NetworksðŸ§ ðŸ”¢","metadata":{}},{"cell_type":"markdown","source":"![](https://media.istockphoto.com/photos/happy-mixed-breed-dog-posing-with-a-kitten-on-his-head-picture-id1210341751?k=20&m=1210341751&s=612x612&w=0&h=v0Dw0214h_1zUG1bZTXx8IG0QwfOwi3Iqd52QNii6ZI=)","metadata":{}},{"cell_type":"markdown","source":"# What is Convolutional Neural Networks??\n\n   **In deep learning, a convolutional neural network is a class of artificial neural network, most commonly applied to analyze visual imagery.**\n \n   \n![](https://miro.medium.com/max/880/1*-9yF-a8gUktDKgRpuNuxNA.png)","metadata":{}},{"cell_type":"markdown","source":"# What are the different layers in the convolutional neural network?\n\nSome of the basic layers in the CNNðŸ‘‡:\n\n1. Convolutional Layer: This layer is the first layer that is used to extract the various features from the input images. In this layer, the mathematical operation of convolution is performed between the input image and a filter of a particular size MxM. By sliding the filter over the input image, the dot product is taken between the filter and the parts of the input image with respect to the size of the filter (MxM).The output is termed as the Feature map which gives us information about the image such as the corners and edges. Later, this feature map is fed to other layers to learn several other features of the input image.\n\n2. Pooling Layer: In most cases, a Convolutional Layer is followed by a Pooling Layer. The primary aim of this layer is to decrease the size of the convolved feature map to reduce the computational costs. This is performed by decreasing the connections between layers and independently operates on each feature map. Depending upon method used, there are several types of Pooling operations. In Max Pooling, the largest element is taken from feature map. Average Pooling calculates the average of the elements in a predefined sized Image section. The total sum of the elements in the predefined section is computed in Sum Pooling. The Pooling Layer usually serves as a bridge between the Convolutional Layer and the FC Layer.\n\n3. Dense Layer: In any neural network, a dense layer is a layer that is deeply connected with its preceding layer which means the neurons of the layer are connected to every neuron of its preceding layer. This layer is the most commonly used layer in artificial neural network networks. The dense layerâ€™s neuron in a model receives output from every neuron of its preceding layer, where neurons of the dense layer perform matrix-vector multiplication. Matrix vector multiplication is a procedure where the row vector of the output from the preceding layers is equal to the column vector of the dense layer. The general rule of matrix-vector multiplication is that the row vector must have as many columns like the column vector.\n\n4. Input layer: The input layer is the input of the whole CNN. In the neural network of image processing, it generally represents the pixel matrix of the image.\n\n5. Output layer: In a CNN as mentioned previously is a fully connected layer, where the input from the other layers is flattened and sent so as the transform the output into the number of classes as desired by the network.","metadata":{}},{"cell_type":"markdown","source":"## Workflow of CNN\n![CNN GIF](https://miro.medium.com/max/1400/1*n3TBO5i8hrYAujlhiHoE_w.gif)","metadata":{}},{"cell_type":"markdown","source":"# What is transfer learning ðŸª„\n\nThe reuse of a previously learned model on a new problem is known as transfer learning. Itâ€™s particularly popular in deep learning right now since it can train deep neural networks with a small amount of data. This is particularly valuable in the field of data science, as most real-world situations do not require millions of labelled data points to train complicated models. \n\n\n## For Deep Learning\n![](https://editor.analyticsvidhya.com/uploads/751191_rsgubd7aTgUdY65KPYATBA.png)\n\n## How it works\n\nIn computer vision, neural networks typically aim to detect edges in the first layer, forms in the middle layer, and task-specific features in the latter layers. The early and central layers are employed in transfer learning, and the latter layers are only retrained. It makes use of the labelled data from the task it was trained on.\n\n## Why Should You Use Transfer Learning?\n\nTransfer learning offers a number of advantages, the most important of which are reduced training time, improved neural network performance (in most circumstances), and the absence of a large amount of data.\n\nTo train a neural model from scratch, a lot of data is typically needed, but access to that data isnâ€™t always possible â€“ this is when transfer learning comes in handy.\n\n![](https://editor.analyticsvidhya.com/uploads/35504classifiers-transfer-learning.jpeg)\n\n## When to Use Transfer Learning\n\nWhen we donâ€™t have enough annotated data to train our model with. When there is a pre-trained model that has been trained on similar data and tasks. If you used TensorFlow to train the original model, you might simply restore it and retrain some layers for your job. Transfer learning, on the other hand, only works if the features learnt in the first task are general, meaning they can be applied to another activity. Furthermore, the modelâ€™s input must be the same size as it was when it was first trained.\n\n- TRAINING A MODEL TO REUSE IT\n\nConsider the situation in which you wish to tackle Task A but lack the necessary data to train a deep neural network. Finding a related task B with a lot of data is one method to get around this.\n\nUtilize the deep neural network to train on task B and then use the model to solve task A. The problem youâ€™re seeking to solve will decide whether you need to employ the entire model or just a few layers.\n\nIf the input in both jobs is the same, you might reapply the model and make predictions for your new input. Changing and retraining distinct task-specific layers and the output layer, on the other hand, is an approach to investigate.\n\n- USING A PRE-TRAINED MODEL\n\nThe second option is to employ a model that has already been trained. There are a number of these models out there, so do some research beforehand. The number of layers to reuse and retrain is determined by the task.\n\n## Models That Have Been Pre-Trained\n\nThere are a number of popular pre-trained machine learning models available. The Inception-v3 model, which was developed for the ImageNet â€œLarge Visual Recognition Challenge,â€ is one of them.â€ Participants in this challenge had to categorize pictures into 1,000 subcategories such as â€œzebra,â€ â€œDalmatian,â€ and â€œdishwasher.â€\n![](https://editor.analyticsvidhya.com/uploads/499849315476_1592890541_transfer.jpg)","metadata":{}},{"cell_type":"markdown","source":"# Lets train the both models (CNN and Transfer Learning)ðŸ¥Š","metadata":{}},{"cell_type":"markdown","source":"### Implementing the code ( Image detection (ðŸˆvsðŸ•) )","metadata":{}},{"cell_type":"markdown","source":"## Importing essential libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport glob\nimport itertools\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:25:41.013002Z","iopub.execute_input":"2022-03-20T15:25:41.013238Z","iopub.status.idle":"2022-03-20T15:25:41.039882Z","shell.execute_reply.started":"2022-03-20T15:25:41.013172Z","shell.execute_reply":"2022-03-20T15:25:41.039242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### File path of the train and test data","metadata":{}},{"cell_type":"code","source":"train_path = '/kaggle/input/cat-and-dog/training_set/training_set'\n# valid_path = 'data/dogs-vs-cats/valid'\ntest_path = '../input/cat-and-dog/test_set/test_set'","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:25:41.041422Z","iopub.execute_input":"2022-03-20T15:25:41.041677Z","iopub.status.idle":"2022-03-20T15:25:41.045463Z","shell.execute_reply.started":"2022-03-20T15:25:41.041644Z","shell.execute_reply":"2022-03-20T15:25:41.044676Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing deep learning libraries ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D,Dropout\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\nimport tensorflow as tf","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:25:41.046722Z","iopub.execute_input":"2022-03-20T15:25:41.047081Z","iopub.status.idle":"2022-03-20T15:25:46.150514Z","shell.execute_reply.started":"2022-03-20T15:25:41.047047Z","shell.execute_reply":"2022-03-20T15:25:46.149775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Augmentation\n\nImage Augmentation for Computer Vision Applications\nAmongst the popular deep learning applications, computer vision tasks such as image classification, object detection, and segmentation have been highly successful. Data augmentation can be effectively used to train the DL models in such applications. Some of the simple transformations applied to the image are; geometric transformations such as Flipping, Rotation, Translation, Cropping, Scaling, and color space transformations such as color casting, Varying brightness, and noise injection.","metadata":{}},{"cell_type":"code","source":"train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg19.preprocess_input) \\\n    .flow_from_directory(directory=train_path, target_size=(224,224), classes=['cats', 'dogs'], batch_size=16)\ntest_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input) \\\n    .flow_from_directory(directory=test_path, target_size=(224,224), classes=['cats', 'dogs'], batch_size=16, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:25:46.152334Z","iopub.execute_input":"2022-03-20T15:25:46.152571Z","iopub.status.idle":"2022-03-20T15:25:52.47851Z","shell.execute_reply.started":"2022-03-20T15:25:46.15254Z","shell.execute_reply":"2022-03-20T15:25:52.477794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Printing the images, size(dimensions) and labels","metadata":{}},{"cell_type":"code","source":"imgs, labels = next(train_batches)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:25:52.479825Z","iopub.execute_input":"2022-03-20T15:25:52.480236Z","iopub.status.idle":"2022-03-20T15:25:52.621413Z","shell.execute_reply.started":"2022-03-20T15:25:52.480198Z","shell.execute_reply":"2022-03-20T15:25:52.620728Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plotImages(images_arr):\n    fig, axes = plt.subplots(1, 10, figsize=(20,20))\n    axes = axes.flatten()\n    for img, ax in zip( images_arr, axes):\n        ax.imshow(img)\n        ax.axis('off')\n        print(img.shape)\n#         print(labels)\n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:25:52.624587Z","iopub.execute_input":"2022-03-20T15:25:52.625012Z","iopub.status.idle":"2022-03-20T15:25:52.632502Z","shell.execute_reply.started":"2022-03-20T15:25:52.624978Z","shell.execute_reply":"2022-03-20T15:25:52.631662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotImages(imgs)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:25:52.636084Z","iopub.execute_input":"2022-03-20T15:25:52.636343Z","iopub.status.idle":"2022-03-20T15:25:53.209087Z","shell.execute_reply.started":"2022-03-20T15:25:52.636317Z","shell.execute_reply":"2022-03-20T15:25:53.208417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:25:53.211729Z","iopub.execute_input":"2022-03-20T15:25:53.211953Z","iopub.status.idle":"2022-03-20T15:25:53.217243Z","shell.execute_reply.started":"2022-03-20T15:25:53.211923Z","shell.execute_reply":"2022-03-20T15:25:53.21645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Point this - cats are labeled with a one-hot encoding of [1,0], and dogs are labeled as [0,1].","metadata":{}},{"cell_type":"markdown","source":"# Building a model\n\nBuilding a neural network with the help of tensorflow and keras. Training CNN model and model with transfer learning, then by comparing them we can see the difference.","metadata":{}},{"cell_type":"markdown","source":"## Convolutional Neural Network","metadata":{}},{"cell_type":"code","source":"model = Sequential()\n\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=(224, 224, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=(224, 224, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n\n# model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n# model.add(BatchNormalization())\n# model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n# model.add(BatchNormalization())\n# model.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPool2D(pool_size=(2, 2), padding='same'))\n\nmodel.add(Flatten())\n# model.add(Dense(64, activation='relu'))\n# model.add(Dropout(0.2))\n# model.add(Dense(64, activation='relu'))\n# model.add(Dropout(0.2))\nmodel.add(Dense(2, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', \n              optimizer='adam', \n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:25:53.218837Z","iopub.execute_input":"2022-03-20T15:25:53.219329Z","iopub.status.idle":"2022-03-20T15:25:55.723098Z","shell.execute_reply.started":"2022-03-20T15:25:53.219292Z","shell.execute_reply":"2022-03-20T15:25:55.72128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:25:55.724466Z","iopub.execute_input":"2022-03-20T15:25:55.724727Z","iopub.status.idle":"2022-03-20T15:25:55.737575Z","shell.execute_reply.started":"2022-03-20T15:25:55.724692Z","shell.execute_reply":"2022-03-20T15:25:55.736183Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Avoid overfitting","metadata":{}},{"cell_type":"code","source":"# Using callbacks for avoiding overfitting\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n\nearly_stopping = EarlyStopping(patience=5, \n                               min_delta=0.001,\n                               restore_best_weights=True)\n\n# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n#                                            patience = 2,\n#                                            verbose=1,\n#                                            factor=0.5,\n#                                            min_lr = 0.00001)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:25:55.738913Z","iopub.execute_input":"2022-03-20T15:25:55.739646Z","iopub.status.idle":"2022-03-20T15:25:55.744718Z","shell.execute_reply.started":"2022-03-20T15:25:55.739593Z","shell.execute_reply":"2022-03-20T15:25:55.743682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fitting the CNN Model","metadata":{}},{"cell_type":"markdown","source":"You can try with more epochs...","metadata":{}},{"cell_type":"code","source":"# fitting the model\nr = model.fit(x=train_batches,epochs=10,callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:25:55.746362Z","iopub.execute_input":"2022-03-20T15:25:55.747095Z","iopub.status.idle":"2022-03-20T15:32:58.057735Z","shell.execute_reply.started":"2022-03-20T15:25:55.747055Z","shell.execute_reply":"2022-03-20T15:32:58.056934Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN model Performance with the help of graphs.","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(2,2,1)\nplt.plot(r.history['loss'], label='Loss')\n# plt.plot(r.history['val_loss'],label='Val_Loss')\nplt.legend()\nplt.title('Loss Evaluation')\n\nplt.subplot(2,2,2)\nplt.plot(r.history['accuracy'], label='Accuracy')\n# plt.plot(r.history['val_accuracy'],label='Val_Accuracy')\nplt.legend()\nplt.title('Accuracy Evolution')","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:32:58.05951Z","iopub.execute_input":"2022-03-20T15:32:58.059829Z","iopub.status.idle":"2022-03-20T15:32:58.375512Z","shell.execute_reply.started":"2022-03-20T15:32:58.059792Z","shell.execute_reply":"2022-03-20T15:32:58.374841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### CNN Model performed very well in this case.","metadata":{}},{"cell_type":"markdown","source":"## CNN model Predictions","metadata":{}},{"cell_type":"markdown","source":"# Test images","metadata":{}},{"cell_type":"code","source":"# Extracting a batch of images and their corresponding labels from the test set.\n\ntest_imgs, test_labels = next(test_batches)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:32:58.376887Z","iopub.execute_input":"2022-03-20T15:32:58.377276Z","iopub.status.idle":"2022-03-20T15:32:58.519564Z","shell.execute_reply.started":"2022-03-20T15:32:58.377239Z","shell.execute_reply":"2022-03-20T15:32:58.518894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plotImages(test_imgs)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:32:58.522429Z","iopub.execute_input":"2022-03-20T15:32:58.522654Z","iopub.status.idle":"2022-03-20T15:32:59.219303Z","shell.execute_reply.started":"2022-03-20T15:32:58.522606Z","shell.execute_reply":"2022-03-20T15:32:59.21865Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(test_labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:32:59.220575Z","iopub.execute_input":"2022-03-20T15:32:59.22096Z","iopub.status.idle":"2022-03-20T15:32:59.226305Z","shell.execute_reply.started":"2022-03-20T15:32:59.220925Z","shell.execute_reply":"2022-03-20T15:32:59.225315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predictions on the test data","metadata":{}},{"cell_type":"code","source":"predictions = model.predict(x=test_batches, verbose=0)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:32:59.227698Z","iopub.execute_input":"2022-03-20T15:32:59.228091Z","iopub.status.idle":"2022-03-20T15:33:15.34889Z","shell.execute_reply.started":"2022-03-20T15:32:59.228056Z","shell.execute_reply":"2022-03-20T15:33:15.348104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.round(predictions)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:15.350323Z","iopub.execute_input":"2022-03-20T15:33:15.350554Z","iopub.status.idle":"2022-03-20T15:33:15.358272Z","shell.execute_reply.started":"2022-03-20T15:33:15.350521Z","shell.execute_reply":"2022-03-20T15:33:15.357365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluating the CNN model with confusion matrix","metadata":{"execution":{"iopub.status.busy":"2022-02-01T10:11:42.346578Z","iopub.execute_input":"2022-02-01T10:11:42.347311Z","iopub.status.idle":"2022-02-01T10:11:42.352446Z","shell.execute_reply.started":"2022-02-01T10:11:42.347275Z","shell.execute_reply":"2022-02-01T10:11:42.350779Z"}}},{"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:15.359556Z","iopub.execute_input":"2022-03-20T15:33:15.360036Z","iopub.status.idle":"2022-03-20T15:33:15.878151Z","shell.execute_reply.started":"2022-03-20T15:33:15.359974Z","shell.execute_reply":"2022-03-20T15:33:15.877419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Above function will return the confusion matrix","metadata":{}},{"cell_type":"code","source":"import itertools\ndef plot_cm(cm, classes,normalize=False, title='Confusion Matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    \n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n            horizontalalignment=\"center\",\n            color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:15.881919Z","iopub.execute_input":"2022-03-20T15:33:15.882121Z","iopub.status.idle":"2022-03-20T15:33:15.892139Z","shell.execute_reply.started":"2022-03-20T15:33:15.882096Z","shell.execute_reply":"2022-03-20T15:33:15.889974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Class indices","metadata":{}},{"cell_type":"code","source":"test_batches.class_indices","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:15.893137Z","iopub.execute_input":"2022-03-20T15:33:15.89372Z","iopub.status.idle":"2022-03-20T15:33:15.901476Z","shell.execute_reply.started":"2022-03-20T15:33:15.89368Z","shell.execute_reply":"2022-03-20T15:33:15.90082Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## We will see the confusion matrix of both the models at the end of the this notebook.","metadata":{"execution":{"iopub.status.busy":"2022-02-01T14:37:54.461722Z","iopub.execute_input":"2022-02-01T14:37:54.462774Z","iopub.status.idle":"2022-02-01T14:37:54.761812Z","shell.execute_reply.started":"2022-02-01T14:37:54.462729Z","shell.execute_reply":"2022-02-01T14:37:54.76081Z"}}},{"cell_type":"markdown","source":"# Using Transfer Learning","metadata":{}},{"cell_type":"markdown","source":"##### We are using VGG19 model for this training.\n\n## What is VGG19:\n**VGG-19 is a convolutional neural network that is 19 layers deep. You can load a pretrained version of the network trained on more than a million images from the ImageNet database. The pretrained network can classify images into 1000 object categories, such as keyboard, mouse, pencil, and many animals. As a result, the network has learned rich feature representations for a wide range of images. The network has an image input size of 224-by-224.**\n![](https://media.geeksforgeeks.org/wp-content/uploads/20200219152327/conv-layers-vgg16.jpg)","metadata":{}},{"cell_type":"markdown","source":"## Loading VGG19 the model","metadata":{}},{"cell_type":"code","source":"vgg19 = tf.keras.applications.vgg19.VGG19()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:15.902799Z","iopub.execute_input":"2022-03-20T15:33:15.903278Z","iopub.status.idle":"2022-03-20T15:33:36.75458Z","shell.execute_reply.started":"2022-03-20T15:33:15.903238Z","shell.execute_reply":"2022-03-20T15:33:36.753031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg19.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:36.755882Z","iopub.execute_input":"2022-03-20T15:33:36.756135Z","iopub.status.idle":"2022-03-20T15:33:36.776524Z","shell.execute_reply.started":"2022-03-20T15:33:36.756102Z","shell.execute_reply":"2022-03-20T15:33:36.775781Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# type of a model\ntype(vgg19)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:36.777936Z","iopub.execute_input":"2022-03-20T15:33:36.778267Z","iopub.status.idle":"2022-03-20T15:33:38.519687Z","shell.execute_reply.started":"2022-03-20T15:33:36.778228Z","shell.execute_reply":"2022-03-20T15:33:38.518876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Weâ€™re going to go through a process to convert the Functional model to a Sequential model, so that it will be easier for us to work with given our current knowledge.\n\nWe first create a new model of type Sequential. We then iterate over each of the layers in vgg19, except for the last layer, and add each layer to the new Sequential model.","metadata":{}},{"cell_type":"code","source":"vgg_model = Sequential()\nfor layer in vgg19.layers[:-1]:\n    vgg_model.add(layer)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:38.521173Z","iopub.execute_input":"2022-03-20T15:33:38.525625Z","iopub.status.idle":"2022-03-20T15:33:38.734047Z","shell.execute_reply.started":"2022-03-20T15:33:38.525579Z","shell.execute_reply":"2022-03-20T15:33:38.733303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"weâ€™ll iterate over each of the layers in our new Sequential model and set them to be non-trainable. This freezes the weights and other trainable parameters in each layer so that they will not be trained or updated when we later pass in our images of cats and dogs.","metadata":{}},{"cell_type":"code","source":"for layer in vgg_model.layers:\n    layer.trainable = False","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:38.735556Z","iopub.execute_input":"2022-03-20T15:33:38.736001Z","iopub.status.idle":"2022-03-20T15:33:38.741588Z","shell.execute_reply.started":"2022-03-20T15:33:38.735964Z","shell.execute_reply":"2022-03-20T15:33:38.74067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The reason we donâ€™t want to retrain these layers is because, as mentioned earlier, cats and dogs were already included in the original ImageNet library. So, VGG16 already does a nice job at classifying these categories. We only want to modify the model such that the output layer understands only how to classify cats and dogs and nothing else. Therefore, we donâ€™t want any re-training to occur on the earlier layers.\n\nNext, we add our new output layer, consisting of only 2 nodes that correspond to cat and dog. This output layer will be the only trainable layer in the model.","metadata":{}},{"cell_type":"code","source":"vgg_model.add(Dense(units=2, activation=\"softmax\"))","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:38.743612Z","iopub.execute_input":"2022-03-20T15:33:38.744045Z","iopub.status.idle":"2022-03-20T15:33:38.772358Z","shell.execute_reply.started":"2022-03-20T15:33:38.744009Z","shell.execute_reply":"2022-03-20T15:33:38.771744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg_model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:38.773545Z","iopub.execute_input":"2022-03-20T15:33:38.773851Z","iopub.status.idle":"2022-03-20T15:33:38.791034Z","shell.execute_reply.started":"2022-03-20T15:33:38.773813Z","shell.execute_reply":"2022-03-20T15:33:38.79032Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Training the VGG19 model","metadata":{}},{"cell_type":"code","source":"vgg_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:38.792379Z","iopub.execute_input":"2022-03-20T15:33:38.792636Z","iopub.status.idle":"2022-03-20T15:33:38.803569Z","shell.execute_reply.started":"2022-03-20T15:33:38.792588Z","shell.execute_reply":"2022-03-20T15:33:38.802938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"early_stopping = EarlyStopping(patience=2, \n                               min_delta=0.001,\n                               restore_best_weights=True)\n\n# learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n#                                            patience = 1,\n#                                            verbose=1,\n#                                            factor=0.5,\n#                                            min_lr = 0.00001)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:38.804886Z","iopub.execute_input":"2022-03-20T15:33:38.805239Z","iopub.status.idle":"2022-03-20T15:33:38.810675Z","shell.execute_reply.started":"2022-03-20T15:33:38.805203Z","shell.execute_reply":"2022-03-20T15:33:38.80868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vgg_model.fit(x = train_batches,\n          steps_per_epoch = len(train_batches),\n          epochs=5,\n          callbacks=[early_stopping])","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:33:38.811652Z","iopub.execute_input":"2022-03-20T15:33:38.812195Z","iopub.status.idle":"2022-03-20T15:36:39.241283Z","shell.execute_reply.started":"2022-03-20T15:33:38.812157Z","shell.execute_reply":"2022-03-20T15:36:39.24064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12,8))\nplt.subplot(2,2,1)\nplt.plot(vgg_model.history.history['loss'], label='Loss')\n# plt.plot(r.history['val_loss'],label='Val_Loss')\nplt.legend()\nplt.title('Loss Evaluation')\n\nplt.subplot(2,2,2)\nplt.plot(vgg_model.history.history['accuracy'], label='Accuracy')\n# plt.plot(r.history['val_accuracy'],label='Val_Accuracy')\nplt.legend()\nplt.title('Accuracy Evolution on VGG19')","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:36:39.242763Z","iopub.execute_input":"2022-03-20T15:36:39.243033Z","iopub.status.idle":"2022-03-20T15:36:39.587102Z","shell.execute_reply.started":"2022-03-20T15:36:39.242998Z","shell.execute_reply":"2022-03-20T15:36:39.586451Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### In both CNN and VGG19 our models trained very well. Both are shwoing same type of trend in their loss and accuracy graphs.","metadata":{}},{"cell_type":"markdown","source":"#### So who won??","metadata":{}},{"cell_type":"markdown","source":"First of all lets evaluate the VGG19 model and that will show us the result will show us the winner. ","metadata":{}},{"cell_type":"markdown","source":"# Prediction on VGG19 model","metadata":{}},{"cell_type":"code","source":"predVGG19 = vgg_model.predict(x=test_batches)\npredVGG19","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:36:39.588443Z","iopub.execute_input":"2022-03-20T15:36:39.588908Z","iopub.status.idle":"2022-03-20T15:36:47.538763Z","shell.execute_reply.started":"2022-03-20T15:36:39.58887Z","shell.execute_reply":"2022-03-20T15:36:47.538041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# rounding off the prediction of VGG19\nnp.round(predVGG19)","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:36:47.54284Z","iopub.execute_input":"2022-03-20T15:36:47.544876Z","iopub.status.idle":"2022-03-20T15:36:47.555601Z","shell.execute_reply.started":"2022-03-20T15:36:47.544812Z","shell.execute_reply":"2022-03-20T15:36:47.554762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### This numbers and arrays are to confusing. So best practice is for evaluation of this model is Confusion Matrix.","metadata":{}},{"cell_type":"markdown","source":"We already made a function that will give us confusion matrix with labels","metadata":{}},{"cell_type":"markdown","source":"### Confusion Matrix on VGG19 model vs CNN model","metadata":{}},{"cell_type":"code","source":"vgg19_cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predVGG19, axis=-1))","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:36:47.559567Z","iopub.execute_input":"2022-03-20T15:36:47.561467Z","iopub.status.idle":"2022-03-20T15:36:47.580459Z","shell.execute_reply.started":"2022-03-20T15:36:47.561425Z","shell.execute_reply":"2022-03-20T15:36:47.579861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix - CNN Model","metadata":{}},{"cell_type":"code","source":"cm_plot_labels = ['cat', 'dog']\nplot_cm(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:36:47.583996Z","iopub.execute_input":"2022-03-20T15:36:47.585866Z","iopub.status.idle":"2022-03-20T15:36:47.885127Z","shell.execute_reply.started":"2022-03-20T15:36:47.585829Z","shell.execute_reply":"2022-03-20T15:36:47.884441Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Confusion Matrix - VGG19 Model","metadata":{}},{"cell_type":"code","source":"cm_plot_labels = ['cat', 'dog']\nplot_cm(cm=vgg19_cm, classes=cm_plot_labels, title='Confusion Matrix')","metadata":{"execution":{"iopub.status.busy":"2022-03-20T15:36:47.886333Z","iopub.execute_input":"2022-03-20T15:36:47.886606Z","iopub.status.idle":"2022-03-20T15:36:48.128688Z","shell.execute_reply.started":"2022-03-20T15:36:47.88657Z","shell.execute_reply":"2022-03-20T15:36:48.127985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# WinnerðŸ¥‡:","metadata":{}},{"cell_type":"markdown","source":"Both the models performed very well but from confusion matrix we saw thet **VGG19**(transfer learning) is few steps ahead that CNN model.\n\n\nI made this notebook while learning Transfer Learning.\n\n\n### Give an upvoteâš¡","metadata":{}}]}